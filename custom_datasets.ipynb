{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM86ybMspbHs8Tr1T/p1wGM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadHelmyOmar/PyTorch_Projects/blob/main/custom_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recap"
      ],
      "metadata": {
        "id": "mCcUgmhR43El"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps to solve a machine learning problem:\n",
        "1. Find a dataset\n",
        "2. Turn the dataset into numbers\n",
        "3. Create a model or find a pretrained one to extract the patterns in the data for prediction"
      ],
      "metadata": {
        "id": "oTdoQWu0STDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro"
      ],
      "metadata": {
        "id": "3N6AP54GtHtG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Custom Data: a collection of data relating to your problem.\n",
        "- PyTorch provides functions to load in different custom datasets in the **TorchVision**, **TorchText**, **TorchAudio**, and **TorchRec** domain libraries.\n",
        "- If the previous functions are not enough, we can subclass **torch.utils.data.Dataset** and customize it to our needs.\n",
        "- In this notebook, we will not use an in-built PyTorch dataset.\n",
        "- We will load a custom dataset and train a model on it.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZeI28jrvtkIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up environment"
      ],
      "metadata": {
        "id": "Q4xG5UcCJV5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "torch.__version__"
      ],
      "metadata": {
        "id": "dbquGT-bJlA2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "585bef08-8698-4b97-c292-7320104299bd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.5.1+cu124'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SKYl1-Xj3UNh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "0af2c81e-71ae-46a8-ce81-1b67a1d433e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Using a device-agnostic code is a best practice in DL\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up Data"
      ],
      "metadata": {
        "id": "JCnUOmmkTnih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Start with existing data.\n",
        "- We will use a subset of the [Food-101](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/) dataset.\n",
        "- Food-101 splits to:\n",
        "    - 1000 images of 101 different kinds of foods = 101,000 total images\n",
        "    - 750,750 for training and 250,250 for testing\n",
        "- Food-101 is included in PyTorch now."
      ],
      "metadata": {
        "id": "ufL3yhUZT68y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Formatting the custom data"
      ],
      "metadata": {
        "id": "YbcWeywwfwXf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We will start with three food classes: Sushi, Pizza, and Steak."
      ],
      "metadata": {
        "id": "kPSKrmxifz0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision as tv\n",
        "\n",
        "tv.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "S_WwLuIkgsNH",
        "outputId": "3ffba097-8439-4dbc-a0e5-abafa9254a38"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.20.1+cu124'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up data directory\n",
        "import pathlib\n",
        "\n",
        "data_dir = pathlib.Path(\"../data\")"
      ],
      "metadata": {
        "id": "6wopWbj5ow6n"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iBfEyeGRpAlg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}